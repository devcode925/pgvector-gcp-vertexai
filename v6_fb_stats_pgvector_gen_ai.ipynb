{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5SwRvvKJvcz"
      },
      "source": [
        "# VertexAI, LangChain and numeric data\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS7GdlJ1XowY"
      },
      "source": [
        "## Example Scenario\n",
        "\n",
        "This notebook uses Facebook dataset which is filled with stats about Facebook Posts. Such as type, date posted, engagement etc.\n",
        "\n",
        "The goals are:\n",
        "\n",
        "- (_Usecase 1_) For marketers: Build a new AI-powered hybrid search, where users can describe their questions in simple English text, along with regular filters (like POST type, etc.)\n",
        "\n",
        "\n",
        "Dataset:\n",
        "- The dataset for this notebook is dataset_facebook2.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OopTDfqckI57"
      },
      "source": [
        "## Overview of the steps\n",
        "\n",
        "1. Download the dataset and load it into a PostgreSQL table called `fb_stats` has the following columns.\n",
        "  \n",
        "`page_total_likes`,`post_type`,`category`,`post_month`,`post_weekday`,`post_hour`,`paid`,`lifetime_post_total_reach`,`lifetime_post_total_impressions`,`lifetime_engaged_users`,`lifetime_post_consumers`,`lifetime_post_consumptions`,\n",
        "`lifetime_post_impressions_by_people_who_liked_your_page`,`lifetime_post_reach_by_people_who_like_your_page`,`lifetime_people_have_liked_your_page_engaged_your_post`,`comments`,`likes`,`shares`,`total_interactions`.\n",
        "\n",
        "2. Make each row its own vector and then into smaller chunks and generate\n",
        "   vector embeddings for each chunk. The vector embeddings are then stored in another PostgreSQL table called `fb_stats_embeddings` using the `pgvector` extension. The `fb_stats_embeddings` table has a foreign key referencing the `fb_stat` table.\n",
        "3. For a given user query, generate its vector embeddings and use `pgvector`\n",
        "   vector similarity search operators to find closest matching fb_stats _after applying the relevant SQL filters._\n",
        "4. Once matching rows and their descriptions are found, use the [MapReduceChain](https://python.langchain.com/docs/modules/chains/document/map_reduce) from LangChain framework to generate a summarized high-quality context using an LLM model (Google PaLM in this case).\n",
        "5. Finally, pass the context to an LLM prompt to answer the user query. The LLM\n",
        "   model will return a well-formatted natural sounding English result back to\n",
        "   the user.\n",
        "   \n",
        "---\n",
        "\n",
        "&nbsp;\n",
        "&nbsp;\n",
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nqImIYGf-yG"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHZVIMDm3djR"
      },
      "source": [
        "### Install required packages\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jI_1BhMx3iX0"
      },
      "outputs": [],
      "source": [
        "# Install dependencies.\n",
        "!pip install asyncio==3.4.3 asyncpg==0.27.0 cloud-sql-python-connector[\"asyncpg\"]==1.2.3\n",
        "!pip install numpy==1.22.4 pandas==1.5.3\n",
        "!pip install pgvector==0.1.8\n",
        "!pip install langchain==0.0.196 transformers==4.30.1\n",
        "!pip install google-cloud-aiplatform==1.26.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4ITjLkp4ME0"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment\n",
        "# can access the new packages.\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX0ssSs11bYz"
      },
      "source": [
        "### Setup Google Cloud environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5Zln92ee1xvG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Please fill in these values.\n",
        "project_id = \"\"  # @param {type:\"string\"}\n",
        "database_password = \"\"  # @param {type:\"string\"}\n",
        "region = \"\"  # @param {type:\"string\"}\n",
        "instance_name = \"\"  # @param {type:\"string\"}\n",
        "database_name = \"\"  # @param {type:\"string\"}\n",
        "database_user = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "# Quick input validations.\n",
        "assert project_id, \"Please provide a Google Cloud project ID\"\n",
        "assert region, \"Please provide a Google Cloud region\"\n",
        "assert instance_name, \"Please provide the name of your instance\"\n",
        "assert database_name, \"Please provide a database name\"\n",
        "assert database_user, \"Please provide a database user\"\n",
        "assert database_password, \" Please provide a database password\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni20S52G2HLi"
      },
      "outputs": [],
      "source": [
        "#@markdown ###Authenticate Account and enable APIs.\n",
        "# Authenticate gcloud.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Configure gcloud.\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "# Grant Cloud SQL Client role to authenticated user\n",
        "current_user = !gcloud auth list --filter=status:ACTIVE --format=\"value(account)\"\n",
        "\n",
        "!gcloud projects add-iam-policy-binding {project_id} \\\n",
        "  --member=user:{current_user[0]} \\\n",
        "  --role=\"roles/cloudsql.client\"\n",
        "\n",
        "\n",
        "# Enable Cloud SQL Admin API\n",
        "!gcloud services enable sqladmin.googleapis.com\n",
        "!gcloud services enable aiplatform.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYE38EHefzjj"
      },
      "source": [
        "### Setup Cloud SQL instance and PostgreSQL database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3UzoWgEelyT"
      },
      "outputs": [],
      "source": [
        "#@markdown Create and setup a Cloud SQL PostgreSQL instance, if not done already.\n",
        "database_version = !gcloud sql instances describe {instance_name} --format=\"value(databaseVersion)\"\n",
        "if database_version[0].startswith(\"POSTGRES\"):\n",
        "  print(\"Found an existing Postgres Cloud SQL Instance!\")\n",
        "else:\n",
        "  print(\"Creating new Cloud SQL instance...\")\n",
        "  !gcloud sql instances create {instance_name} --database-version=POSTGRES_15 \\\n",
        "    --region={region} --cpu=1 --memory=4GB --root-password={database_password}\n",
        "\n",
        "# Create the database, if it does not exist.\n",
        "out = !gcloud sql databases list --instance={instance_name} --filter=\"NAME:{database_name}\" --format=\"value(NAME)\"\n",
        "if ''.join(out) == database_name:\n",
        "  print(\"Database %s already exists, skipping creation.\" % database_name)\n",
        "else:\n",
        "  !gcloud sql databases create {database_name} --instance={instance_name}\n",
        "\n",
        "# Create the database user for accessing the database.\n",
        "!gcloud sql users create {database_user} \\\n",
        "  --instance={instance_name} \\\n",
        "  --password={database_password}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "h9f8iQAXfdai"
      },
      "outputs": [],
      "source": [
        "# @markdown Verify that you are able to connect to the database. Executing this block should print the current PostgreSQL server version.\n",
        "\n",
        "import asyncio\n",
        "import asyncpg\n",
        "from google.cloud.sql.connector import Connector\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # get current running event loop to be used with Connector\n",
        "    loop = asyncio.get_running_loop()\n",
        "    # initialize Connector object as async context manager\n",
        "    async with Connector(loop=loop) as connector:\n",
        "        # create connection to Cloud SQL database\n",
        "        conn: asyncpg.Connection = await connector.connect_async(\n",
        "            f\"{project_id}:{region}:{instance_name}\",  # Cloud SQL instance connection name\n",
        "            \"asyncpg\",\n",
        "            user=f\"{database_user}\",\n",
        "            password=f\"{database_password}\",\n",
        "            db=f\"{database_name}\"\n",
        "            # ... additional database driver args\n",
        "        )\n",
        "\n",
        "        # query Cloud SQL database\n",
        "        results = await conn.fetch(\"SELECT * from fb_stats\")\n",
        "        print(results[1])\n",
        "\n",
        "        # close asyncpg connection\n",
        "        await conn.close()\n",
        "\n",
        "\n",
        "# Test connection with `asyncio`\n",
        "await main()  # type: ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj5N8V1CgLJ4"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCO82M0i6TiD"
      },
      "source": [
        "### Download and load the dataset in PostgreSQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYaxNic_DIL6"
      },
      "outputs": [],
      "source": [
        "# Load dataset from content and store it in a pandas dataframe.\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "DATASET_URL =\"/content/dataset_Facebook2.csv\"\n",
        "df = pd.read_csv(DATASET_URL)\n",
        "\n",
        "df = df.dropna()\n",
        "df.head(1)\n",
        "#converting floats to ints\n",
        "df.columns =['page_total_likes','post_type','category','post_month','post_weekday','post_hour','paid','lifetime_post_total_reach','lifetime_post_total_impressions','lifetime_engaged_users','lifetime_post_consumers','lifetime_post_consumptions',\n",
        "'lifetime_post_impressions_by_people_who_liked_your_page','lifetime_post_reach_by_people_who_like_your_page','lifetime_people_have_liked_your_page_engaged_your_post','comments','likes','shares','total_interactions']\n",
        "\n",
        "\n",
        "#df['post_type'] = [str(x) for x in df['post_type']]\n",
        "\n",
        "#df.post_type.describe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GL8reSa-_sUQ"
      },
      "outputs": [],
      "source": [
        "df['row_id'] = df.index\n",
        "#df.rename(columns={'row_d': 'row_id'}, inplace=True)\n",
        "df['row_id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVRlH9cpMuuJ"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amOgB-9yJ-jf"
      },
      "outputs": [],
      "source": [
        "# Save the Pandas dataframe in a PostgreSQL table.\n",
        "\n",
        "import asyncio\n",
        "import asyncpg\n",
        "from google.cloud.sql.connector import Connector\n",
        "\n",
        "\n",
        "async def main():\n",
        "    loop = asyncio.get_running_loop()\n",
        "    async with Connector(loop=loop) as connector:\n",
        "        # Create connection to Cloud SQL database\n",
        "        conn: asyncpg.Connection = await connector.connect_async(\n",
        "            f\"{project_id}:{region}:{instance_name}\",  # Cloud SQL instance connection name\n",
        "            \"asyncpg\",\n",
        "            user=f\"{database_user}\",\n",
        "            password=f\"{database_password}\",\n",
        "            db=f\"{database_name}\",\n",
        "        )\n",
        "     # do we delete old one now?\n",
        "       #await conn.execute(\"DROP TABLE IF EXISTS fb_stats CASCADE\")\n",
        "        # Create the `fb_stats` table.\n",
        "        await conn.execute(\n",
        "            \"\"\"CREATE TABLE fb_stats(\n",
        "                                row_id INTEGER PRIMARY KEY,\n",
        "                                page_total_likes NUMERIC,\n",
        "post_type VARCHAR,category NUMERIC,\n",
        "post_month NUMERIC,post_weekday NUMERIC,post_hour NUMERIC,\n",
        "paid NUMERIC(10,4),lifetime_post_total_reach NUMERIC,lifetime_post_total_impressions NUMERIC,\n",
        "lifetime_engaged_users NUMERIC,lifetime_post_consumers NUMERIC,lifetime_post_consumptions NUMERIC,\n",
        "lifetime_post_impressions_by_people_who_liked_your_page NUMERIC,\n",
        "lifetime_post_reach_by_people_who_like_your_page NUMERIC,\n",
        "lifetime_people_have_liked_your_page_engaged_your_post NUMERIC,\n",
        "comments NUMERIC,likes NUMERIC(10,4),shares NUMERIC(10,4),total_interactions NUMERIC)\"\"\"\n",
        "        )\n",
        "\n",
        "\n",
        "        await conn.close()\n",
        "\n",
        "\n",
        "# Run the SQL commands now.\n",
        "await main()  # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfy0J9Qj7e6S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmqIHhla2JyU"
      },
      "outputs": [],
      "source": [
        "#load the dataframe to the `fb_stats` table.\n",
        "import pandas as pd\n",
        "import psycopg2\n",
        "import asyncio\n",
        "import asyncpg\n",
        "from google.cloud.sql.connector import Connector\n",
        "\n",
        "async def load():\n",
        "    # get current running event loop to be used with Connector\n",
        "    loop = asyncio.get_running_loop()\n",
        "    # initialize Connector object as async context manager\n",
        "    async with Connector(loop=loop) as connector:\n",
        "        # create connection to Cloud SQL database\n",
        "        conn: asyncpg.Connection = await connector.connect_async(\n",
        "            f\"{project_id}:{region}:{instance_name}\",  # Cloud SQL instance connection name\n",
        "            \"asyncpg\",\n",
        "            user=f\"{database_user}\",\n",
        "            password=f\"{database_password}\",\n",
        "            db=f\"{database_name}\"\n",
        "            # ... additional database driver args\n",
        "        )\n",
        "        # query Cloud SQL database\n",
        "        # Load the dataframe into the PostgreSQL table\n",
        "\n",
        "        cols = list(df.columns)\n",
        "        tuples = [tuple(row) for row in df.values]\n",
        "\n",
        "        await conn.copy_records_to_table(\n",
        "            \"fb_stats\", records=tuples, columns=list(df), timeout=10\n",
        "        )\n",
        "\n",
        "        # close asyncpg connection\n",
        "        await conn.close()\n",
        "\n",
        "\n",
        "# Test connection with `asyncio`\n",
        "await load()  # type: ignore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfuVTwRQDIUw"
      },
      "outputs": [],
      "source": [
        "# get the month\n",
        "def get_month(month):\n",
        "  if month == 1: return 'January'\n",
        "  elif month == 2: return 'Feburary'\n",
        "  elif month == 3: return 'March'\n",
        "  elif month == 4: return 'April'\n",
        "  elif month == 5: return 'May'\n",
        "  elif month == 6: return 'June'\n",
        "  elif month == 7: return 'July'\n",
        "  elif month == 8: return 'August'\n",
        "  elif month == 9: return 'September'\n",
        "  elif month == 10: return 'October'\n",
        "  elif month == 11: return 'November'\n",
        "  elif month == 12: return 'December'\n",
        "\n",
        "# get the day of the week\n",
        "def get_weekday(day):\n",
        "  if day == 1: return 'Sunday'\n",
        "  elif day == 2: return 'Monday'\n",
        "  elif day == 3: return 'Tuesday'\n",
        "  elif day == 4: return 'Wednesday'\n",
        "  elif day == 5: return 'Thursday'\n",
        "  elif day == 6: return 'Friday'\n",
        "  elif day == 7: return 'Saturday'\n",
        "\n",
        "# lets add column name and values to one string\n",
        "def create_string_from_row(df, row_index):\n",
        "  \"\"\"Creates a string for a row in a Pandas DataFrame, prepended with the column name.\n",
        "\n",
        "  Args:\n",
        "    df: The Pandas DataFrame.\n",
        "    row_index: The index of the row to be processed.\n",
        "\n",
        "  Returns:\n",
        "    A string representing the row, with a description of the value.\n",
        "  \"\"\"\n",
        "\n",
        "  #column_names = df.columns\n",
        "  #row_values = df.iloc[row_index].tolist()\n",
        "\n",
        "  string = \"\"\n",
        "  '''for i, value in enumerate(row_values):\n",
        "    column_name = column_names[i]\n",
        "    string += f\"{column_name}={value}; \"\n",
        "'''\n",
        "  string += f\"Page Total likes is {df.page_total_likes[row_index]}.The type of Post is {df.post_type[row_index]}.\"\n",
        "  string += f\"The Category is {df.category[row_index]}. The Month the Post was made is {get_month(df.post_month[row_index])}.\"\n",
        "  string += f\"Was it a paid Post {False if df.paid[row_index] == 0 else True}.The Lifetime Post Total Reach is {df.lifetime_post_total_reach[row_index]} viewers.\"\n",
        "  string += f\"The Lifetime Post Total Impressions for this Post is {df.lifetime_post_total_impressions[row_index]}.The Lifetime Engaged Users for this Post is {df.lifetime_engaged_users[row_index]}.\"\n",
        "  string += f\"The Lifetime Post Consumers for this Post is {df.lifetime_post_consumers[row_index]}.The Lifetime Post Consumptions for this Post is {df.lifetime_post_consumptions[row_index]}.\"\n",
        "  string += f\"The Lifetime Post Impressions by people who have liked your Page is {df.lifetime_post_impressions_by_people_who_liked_your_page[row_index]}.The Lifetime Post reach by people who like your Page {df.lifetime_post_reach_by_people_who_like_your_page[row_index]}\"\n",
        "  string += f\"The Lifetime People who have liked your Page and engaged with your post is {df.lifetime_people_have_liked_your_page_engaged_your_post[row_index]}.Comments for this Post is {df.comments[row_index]}.\"\n",
        "  string += f\"The Like count for this Post is {df.comments[row_index]}.How many People shared this Post is {df.shares[row_index]}.\"\n",
        "  string += f\"The Total Interactions for this Post is {df.total_interactions[row_index]}.\"\n",
        "\n",
        "  return string\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  string = create_string_from_row(df, 1)\n",
        "\n",
        "  print(string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP9MDFiIgVoV"
      },
      "source": [
        "## Vector Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zutD18TzudB"
      },
      "source": [
        "### Generate vector embeddings using a Text Embedding model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EphlPxXnMTf"
      },
      "source": [
        "Step 1: Split long stats description text into smaller chunks\n",
        "\n",
        "- The stats descriptions can be much longer than what can fit into a single API request for generating the vector embedding.\n",
        "\n",
        "- For example, Vertex AI text embedding model accepts a maximum of 3,072 input tokens for a single API request.\n",
        "\n",
        "- Use the `RecursiveCharacterTextSplitter` from LangChain library to split\n",
        "the description into smaller chunks of 500 characters each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_geQnFh0XML"
      },
      "outputs": [],
      "source": [
        "# Split long text descriptions into smaller chunks that can fit into\n",
        "# the API request size limit, as expected by the LLM providers.\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\".\", \"\\n\"],\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=0,\n",
        "    length_function=len,\n",
        ")\n",
        "chunked = []\n",
        "cnt =0;\n",
        "for index, row in df.iterrows():\n",
        "    row_id = row[\"row_id\"]\n",
        "    desc = create_string_from_row(df,row_id)\n",
        "    print(cnt, desc)\n",
        "    cnt= cnt + 1\n",
        "    splits = text_splitter.create_documents([desc])\n",
        "    for s in splits:\n",
        "        r = {\"row_id\": row_id, \"content\": s.page_content}\n",
        "        chunked.append(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzrarngXnkOk"
      },
      "source": [
        "Step 2: Generate vector embedding for each chunk by calling an Embedding Generation service\n",
        "\n",
        "-Vertex AI text embedding model is used to generate vector embeddings, which outputs a 768-dimensional vector for each chunk of text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsK_9ARUHTIx"
      },
      "outputs": [],
      "source": [
        "# Generate the vector embeddings for each chunk of text.\n",
        "# This code snippet may run for a few minutes.\n",
        "\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "from google.cloud import aiplatform\n",
        "import time\n",
        "\n",
        "aiplatform.init(project=f\"{project_id}\", location=f\"{region}\")\n",
        "embeddings_service = VertexAIEmbeddings()\n",
        "\n",
        "\n",
        "# Helper function to retry failed API requests with exponential backoff.\n",
        "def retry_with_backoff(func, *args, retry_delay=5, backoff_factor=2, **kwargs):\n",
        "    max_attempts = 10\n",
        "    retries = 0\n",
        "    for i in range(max_attempts):\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            print(f\"error: {e}\")\n",
        "            retries += 1\n",
        "            wait = retry_delay * (backoff_factor**retries)\n",
        "            print(f\"Retry after waiting for {wait} seconds...\")\n",
        "            time.sleep(wait)\n",
        "\n",
        "\n",
        "batch_size = 5\n",
        "for i in range(0, len(chunked), batch_size):\n",
        "    request = [x[\"content\"] for x in chunked[i : i + batch_size]]\n",
        "    response = retry_with_backoff(embeddings_service.embed_documents, request)\n",
        "    # Store the retrieved vector embeddings for each chunk back.\n",
        "    for x, e in zip(chunked[i : i + batch_size], response):\n",
        "        x[\"embedding\"] = e\n",
        "\n",
        "# Store the generated embeddings in a pandas dataframe.\n",
        "fb_stats_embeddings = pd.DataFrame(chunked)\n",
        "fb_stats_embeddings.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f68jtfnNqck"
      },
      "source": [
        "### Use pgvector to store the generated embeddings within PostgreSQL\n",
        "\n",
        "- The `pgvector` extension introduces a new `vector` data type.\n",
        "- **The new `vector` data type allows you to directly save a vector embedding (represented as a NumPy array) through a simple INSERT statement in PostgreSQL!**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZKe_9qeRdMH"
      },
      "outputs": [],
      "source": [
        "# Store the generated vector embeddings in a PostgreSQL table.\n",
        "# This code may run for a few minutes.\n",
        "\n",
        "import asyncio\n",
        "import asyncpg\n",
        "from google.cloud.sql.connector import Connector\n",
        "import numpy as np\n",
        "from pgvector.asyncpg import register_vector\n",
        "\n",
        "\n",
        "async def main():\n",
        "    loop = asyncio.get_running_loop()\n",
        "    async with Connector(loop=loop) as connector:\n",
        "        # Create connection to Cloud SQL database.\n",
        "        conn: asyncpg.Connection = await connector.connect_async(\n",
        "            f\"{project_id}:{region}:{instance_name}\",  # Cloud SQL instance connection name\n",
        "            \"asyncpg\",\n",
        "            user=f\"{database_user}\",\n",
        "            password=f\"{database_password}\",\n",
        "            db=f\"{database_name}\",\n",
        "        )\n",
        "\n",
        "        await conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
        "        await register_vector(conn)\n",
        "\n",
        "        await conn.execute(\"DROP TABLE IF EXISTS fb_stats_embeddings\")\n",
        "        # Create the `fb_stats_embeddings` table to store vector embeddings.\n",
        "        await conn.execute(\n",
        "            \"\"\"CREATE TABLE fb_stats_embeddings(\n",
        "                                row_id INTEGER NOT NULL REFERENCES fb_stats(row_id),\n",
        "                                content TEXT,\n",
        "                                embedding vector(768))\"\"\"\n",
        "        )\n",
        "\n",
        "        # Store all the generated embeddings back into the database.\n",
        "        for index, row in fb_stats_embeddings.iterrows():\n",
        "            await conn.execute(\n",
        "                \"INSERT INTO fb_stats_embeddings (row_id, content, embedding) VALUES ($1, $2, $3)\",\n",
        "                row[\"row_id\"],\n",
        "                row[\"content\"],\n",
        "                np.array(row[\"embedding\"]),\n",
        "            )\n",
        "\n",
        "        await conn.close()\n",
        "\n",
        "\n",
        "# Run the SQL commands now.\n",
        "await main()  # type: ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm0dVJeInyfM"
      },
      "source": [
        "### Finding similar row ids using pgvector cosine search operator - not using, it didn't work consistently enough. and not enough data returned.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "_zRBR9YJoENp"
      },
      "outputs": [],
      "source": [
        "#didn't work, not used\n",
        "#@markdown Enter your question about the Facebook data:\n",
        "fb_stat = \"lifetime_post_impressions_by_people_who_liked_your_page\"  # @param {type:\"string\"}\n",
        "\n",
        "# Quick input validations.\n",
        "assert fb_stat, \"⚠️ Please input a valid input search text\"\n",
        "\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "from pgvector.asyncpg import register_vector\n",
        "import asyncio\n",
        "import asyncpg\n",
        "from google.cloud.sql.connector import Connector\n",
        "\n",
        "aiplatform.init(project=f\"{project_id}\", location=f\"{region}\")\n",
        "\n",
        "embeddings_service = VertexAIEmbeddings()\n",
        "qe = embeddings_service.embed_query([fb_stat])\n",
        "\n",
        "matches = []\n",
        "\n",
        "\n",
        "async def main():\n",
        "    loop = asyncio.get_running_loop()\n",
        "    async with Connector(loop=loop) as connector:\n",
        "        # Create connection to Cloud SQL database.\n",
        "        conn: asyncpg.Connection = await connector.connect_async(\n",
        "            f\"{project_id}:{region}:{instance_name}\",  # Cloud SQL instance connection name\n",
        "            \"asyncpg\",\n",
        "            user=f\"{database_user}\",\n",
        "            password=f\"{database_password}\",\n",
        "            db=f\"{database_name}\",\n",
        "        )\n",
        "\n",
        "        await register_vector(conn)\n",
        "        similarity_threshold = 0.1\n",
        "        num_matches = 50\n",
        "\n",
        "        # Find similar data to the query using cosine similarity search\n",
        "        # over all vector embeddings. This new feature is provided by `pgvector`.\n",
        "        results = await conn.fetch(\n",
        "            \"\"\"\n",
        "                            WITH vector_matches AS (\n",
        "                              SELECT row_id, 1 - (embedding <=> $1) AS similarity\n",
        "                              FROM fb_stats_embeddings\n",
        "                              WHERE 1 - (embedding <=> $1) > $2\n",
        "                              ORDER BY similarity DESC\n",
        "                              LIMIT $3\n",
        "                            )\n",
        "                            SELECT * FROM fb_stats\n",
        "                            WHERE row_id IN (SELECT row_id FROM vector_matches)\n",
        "                            \"\"\",\n",
        "            qe,\n",
        "            similarity_threshold,\n",
        "            num_matches,\n",
        "        )\n",
        "\n",
        "        if len(results) == 0:\n",
        "            raise Exception(\"Did not find any results. Adjust the query parameters.\")\n",
        "\n",
        "        for r in results:\n",
        "            # Collect the row id.\n",
        "            matches.append(\n",
        "                {\n",
        "                    \"row_id\": r[\"row_id\"],\n",
        "                }\n",
        "            )\n",
        "\n",
        "        await conn.close()\n",
        "\n",
        "\n",
        "# Run the SQL commands now.\n",
        "await main()  # type: ignore\n",
        "\n",
        "# Show the results for similar stats that matched the user query.\n",
        "matches = pd.DataFrame(matches)\n",
        "matches.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtDT5DbCNWoe"
      },
      "source": [
        "## LLMs and LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSk0E3hlXSP0"
      },
      "source": [
        "### *Use case 1*: Building an AI-curated contextual hybrid search\n",
        "\n",
        "Combine natural language query text with regular relational filters to create a powerful hybrid search.\n",
        "\n",
        "Example: A grandparent wants to use the **AI-powered search interface** to find an educational toy for their grandkid that fits within their budget."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0-_AYhlgXYSK"
      },
      "outputs": [],
      "source": [
        "# @markdown Enter the user search query in a simple English text. T\n",
        "user_query = \"what combination of photos and images will maximize impressions?\"  # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "# Quick input validations.\n",
        "assert user_query, \"⚠️ Please input a valid input search text\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y83GRy7jdYRa"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=f\"{project_id}\", location=f\"{region}\")\n",
        "\n",
        "embeddings_service = VertexAIEmbeddings()\n",
        "\n",
        "qe = embeddings_service.embed_query([user_query])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsOl5A9ldRgh"
      },
      "source": [
        "Step 1: Generate the vector embedding for the user query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovmRE0updh16"
      },
      "source": [
        "Step 2: Use `pgvector` to find the values in the vector embeddings.\n",
        "\n",
        "- The new `pgvector` similarity search operators provide powerful semantics\n",
        "to combine the vector search operation with regular query filters in a single SQL query.\n",
        "- **Using pgvector, integrate relational databases with vector search operations**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJyqPE9XYCya"
      },
      "outputs": [],
      "source": [
        "from pgvector.asyncpg import register_vector\n",
        "import asyncio\n",
        "import asyncpg\n",
        "from google.cloud.sql.connector import Connector\n",
        "\n",
        "matches = []\n",
        "\n",
        "\n",
        "async def main():\n",
        "    loop = asyncio.get_running_loop()\n",
        "    async with Connector(loop=loop) as connector:\n",
        "        # Create connection to Cloud SQL database.\n",
        "        conn: asyncpg.Connection = await connector.connect_async(\n",
        "            f\"{project_id}:{region}:{instance_name}\",  # Cloud SQL instance connection name\n",
        "            \"asyncpg\",\n",
        "            user=f\"{database_user}\",\n",
        "            password=f\"{database_password}\",\n",
        "            db=f\"{database_name}\",\n",
        "        )\n",
        "\n",
        "        await register_vector(conn)\n",
        "        similarity_threshold = 0.5\n",
        "        num_matches = 5\n",
        "\n",
        "        # Find similar stat descriptions to the query using cosine similarity search\n",
        "        # over all vector embeddings. This new feature is provided by `pgvector`.\n",
        "        results = await conn.fetch(\n",
        "            \"\"\"\n",
        "                            WITH vector_matches AS (\n",
        "                              SELECT row_id, 1 - (embedding <=> $1) AS similarity\n",
        "                              FROM fb_stats_embeddings\n",
        "                              WHERE 1 - (embedding <=> $1) > $2\n",
        "                              ORDER BY similarity DESC\n",
        "                              LIMIT $3\n",
        "                            )\n",
        "                            SELECT * FROM fb_stats\n",
        "                            WHERE row_id IN (SELECT row_id FROM vector_matches)\n",
        "\n",
        "                            \"\"\",\n",
        "            qe,\n",
        "            similarity_threshold,\n",
        "            num_matches,\n",
        "        )\n",
        "\n",
        "        if len(results) == 0:\n",
        "            raise Exception(\"I don't have a good answer. Try rewording the question.\")\n",
        "\n",
        "        for r in results:\n",
        "            # Collect the description for all the matched similar stat descriptions.\n",
        "            matches.append(\n",
        "                f\"\"\"The row id {r[\"row_id\"]}.\n",
        "                          Its description is below:\n",
        "                          {r}.\"\"\"\n",
        "            )\n",
        "        await conn.close()\n",
        "\n",
        "\n",
        "# Run the SQL commands now.\n",
        "await main()  # type: ignore\n",
        "\n",
        "# Show the results for similar stats that matched the user query.\n",
        "#matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oiqKZBKe4jq"
      },
      "source": [
        "Step 3: Use LangChain to summarize and generate a high-quality prompt to answer the user query\n",
        "\n",
        "- After finding the similar rows and their fields using `pgvector`, the next step is to use them for generating a prompt input for the LLM model.\n",
        "- Since individual rows fields can be very long, they may not fit within the specified input payload limit for an LLM model.\n",
        "- The `MapReduceChain` from LangChain framework is used to generate and combine short summaries of similarly matched rows.\n",
        "- The combined summaries are then used to build a high-quality prompt for an input to the LLM model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPPUSBKb1Soc"
      },
      "outputs": [],
      "source": [
        "# Using LangChain for summarization and efficient context building.\n",
        "\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.llms import VertexAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "llm = VertexAI()\n",
        "\n",
        "map_prompt_template = \"\"\"\n",
        "              You will be given a question about the data in the vector embeddings.\n",
        "              the data in the embeddings are Facebook statstics for Posts.\n",
        "               Select the right column described in the question and do the analysis.\n",
        "                You should use SUM() and AVG(), MEAN() when necessary.Always return the row_id. The question will be enclosed in triple backticks (```).\n",
        "              Using this description only, extract the number for the column referenced and return your answer.\n",
        "              ```{text}```\n",
        "              SUMMARY:\n",
        "              \"\"\"\n",
        "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
        "\n",
        "combine_prompt_template = \"\"\"\n",
        "               You will be given a question about the Facebook statistics about POSTs.\n",
        "               All information for answering the question is in the vector embeddings and fb_stats database. (```) and a question enclosed in\n",
        "                double backticks(``).\n",
        "                Select the right column described in the question and do the analysis.\n",
        "                You should use SUM() and AVG(), MEAN() when necessary.\n",
        "                You should only use the information in the description. always return the row_id.\n",
        "                Your answer should include the explanantion for your answer. Your answer should be less than 200 words.\n",
        "                Your answer should be in Markdown in a numbered list format.\n",
        "\n",
        "\n",
        "                Description:\n",
        "                ```{text}```\n",
        "\n",
        "\n",
        "                Question:\n",
        "                ``{user_query}``\n",
        "\n",
        "\n",
        "                Answer:\n",
        "                \"\"\"\n",
        "combine_prompt = PromptTemplate(\n",
        "    template=combine_prompt_template, input_variables=[\"text\", \"user_query\"]\n",
        ")\n",
        "\n",
        "docs = [Document(page_content=t) for t in matches]\n",
        "chain = load_summarize_chain(\n",
        "    llm, chain_type=\"map_reduce\", map_prompt=map_prompt, combine_prompt=combine_prompt\n",
        ")\n",
        "answer = chain.run(\n",
        "    {\n",
        "        \"input_documents\": docs,\n",
        "        \"user_query\": user_query,\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "display(answer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Lm0dVJeInyfM"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
